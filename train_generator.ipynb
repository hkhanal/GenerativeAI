{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Load dataset**:  Load pumed abstract dataset for training. Label does not requires to train the gnerator.\n",
        "\n",
        "**Preprocess the data**: Clean and preprocess the dataset to remove any unnecessary characters, formatting, or noise. Tokenize the text by splitting it into individual words or subwords. This step helps the model understand the structure of the text.\n",
        "\n",
        "**Build the model architecture**: Design the architecture of the text generator model using the pytorch\n",
        "\n",
        "**Train the model**: Feed the preprocessed dataset into the model and train. The training process involves optimizing the model's parameters by minimizing a loss function that measures the difference between the generated text and the ground truth text in the dataset.\n",
        "\n",
        "**Predict**:  Use predict function to generate the texts from the given input texts.\n",
        "\n",
        "**Save and Load model**: Optional"
      ],
      "metadata": {
        "id": "w8VnEjw-zkKL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g66WWj_4hn7e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "drive.mount('gdrive')\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load data, text file from data-directory"
      ],
      "metadata": {
        "id": "orOXfw5dAZYk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5kaOF1BhyHB"
      },
      "outputs": [],
      "source": [
        "\n",
        "data_dir = \"/content/gdrive/My Drive/DagDataScienceMaterial/data_folder/TextFolder/\"\n",
        "dd_file = \"pubmed_abs.csv\"\n",
        "df = pd.read_csv(os.path.join(data_dir, dd_file))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2u6eEtf3i5XT"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Densign model**: We define our neural network by subclassing nn.Module, and initialize the neural network layers in __init__. Every nn.Module subclass implements the operations on input data in the **forward** method."
      ],
      "metadata": {
        "id": "4xdNXxJ1BcCg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wO7jROWmaQFl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import argparse\n",
        "import numpy as np\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "from collections import Counter\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, dataset):\n",
        "        super(Model, self).__init__()\n",
        "        self.lstm_size = 128\n",
        "        self.embedding_dim = 128\n",
        "        self.num_layers = 3\n",
        "\n",
        "        n_vocab = len(dataset.uniq_words)\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=n_vocab,\n",
        "            embedding_dim=self.embedding_dim,\n",
        "        )\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.lstm_size,\n",
        "            hidden_size=self.lstm_size,\n",
        "            num_layers=self.num_layers,\n",
        "            dropout=0.2,\n",
        "        )\n",
        "        self.fc1 = nn.Linear(self.lstm_size, 256)\n",
        "        self.fc2 = nn.Linear(256, n_vocab)\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        embed = self.embedding(x)\n",
        "        output, state = self.lstm(embed, prev_state)\n",
        "        output = self.fc1(output)\n",
        "        logits = self.fc2(output)\n",
        "\n",
        "        return logits, state\n",
        "\n",
        "    def init_state(self, sequence_length):\n",
        "        return (torch.zeros(self.num_layers, sequence_length, self.lstm_size),\n",
        "                torch.zeros(self.num_layers, sequence_length, self.lstm_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8CqU0xyonqp"
      },
      "outputs": [],
      "source": [
        "# Asigning the GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LugvkW4-bSBt"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        args,\n",
        "    ):\n",
        "        self.args = args\n",
        "        self.words = self.load_words()\n",
        "        self.uniq_words = self.get_uniq_words()\n",
        "\n",
        "        self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n",
        "        self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n",
        "\n",
        "        self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
        "\n",
        "    def load_words(self):\n",
        "        text = df['Abstract'].str.cat(sep=' ')\n",
        "        return text.split(' ')\n",
        "\n",
        "    def get_uniq_words(self):\n",
        "        word_counts = Counter(self.words)\n",
        "        return sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.words_indexes) - self.args.sequence_length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return (\n",
        "            torch.tensor(self.words_indexes[index:index+self.args.sequence_length]),\n",
        "            torch.tensor(self.words_indexes[index+1:index+self.args.sequence_length+1]),)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing, tokenizing and indecing in tokens"
      ],
      "metadata": {
        "id": "heW6qf7-blzP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnbt-9XKaduM"
      },
      "outputs": [],
      "source": [
        "def train(dataset, model, args):\n",
        "    model.train()\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=args.batch_size,\n",
        "    )\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(args.max_epochs):\n",
        "        state_h, state_c = model.init_state(args.sequence_length)\n",
        "        total_loss = 0.0\n",
        "        n = 0\n",
        "        model_path = os.path.join(data_dir, args.model_file)\n",
        "        state_h, state_c = state_h.to(device), state_c.to(device)\n",
        "        for batch, (x, y) in enumerate(dataloader):\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
        "            loss = criterion(y_pred.transpose(1, 2), y)\n",
        "\n",
        "            state_h = state_h.detach()\n",
        "            state_c = state_c.detach()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss +=loss.item()\n",
        "            n +=1\n",
        "        total_loss = total_loss/float(n)\n",
        "        if (epoch+1)%5 ==0:\n",
        "          state = {\n",
        "          'epoch': epoch,\n",
        "          'state_dict': model.state_dict(),\n",
        "          'optimizer': optimizer.state_dict()\n",
        "            }\n",
        "          torch.save(state, model_path)\n",
        "\n",
        "          print({ 'epoch': epoch, 'loss': total_loss})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once model is trained, we can generate the new texts from the given inputs"
      ],
      "metadata": {
        "id": "j8LI6Ms5b-Hy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_W2xeV0bN_E"
      },
      "outputs": [],
      "source": [
        "def predict(dataset, model, text, next_words=20):\n",
        "    words = text.split(' ')\n",
        "    model.eval()\n",
        "\n",
        "    state_h, state_c = model.init_state(len(words))\n",
        "    state_h, state_c = state_h.to(device), state_c.to(device)\n",
        "    for i in range(0, next_words):\n",
        "        x = torch.tensor([[dataset.word_to_index[w] for w in words[i:]]])\n",
        "        x = x.to(device)\n",
        "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
        "        last_word_logits = y_pred[0][-1]\n",
        "        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().cpu().numpy()\n",
        "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
        "        words.append(dataset.index_to_word[word_index])\n",
        "\n",
        "    return \" \".join(words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1qNJNsJkccZX"
      },
      "outputs": [],
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--max-epochs', type=int, default=80)\n",
        "parser.add_argument('--batch-size', type=int, default=256)\n",
        "parser.add_argument('--sequence-length', type=int, default=20)\n",
        "parser.add_argument('--model_file', type=str, default=\"pubmed_generator.pt\")\n",
        "args, unknown = parser.parse_known_args()\n",
        "print (args)\n",
        "dataset = Dataset(args)\n",
        "\n",
        "model = Model(dataset)\n",
        "model.to(device)\n",
        "\n",
        "train(dataset, model, args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDXtsXcrbylu"
      },
      "outputs": [],
      "source": [
        "print(predict(dataset, model, text='Most occasions cases for chronic pain', next_words =30))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPRVYqkWgAzN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}