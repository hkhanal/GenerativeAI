{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de004b47-8c30-470d-8e3d-ac9dd0c4ad2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torch import nn\n",
    "import zipfile\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "import pandas  as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60988332-e04a-4c9d-9488-22032051fdc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c23ce9a651f4dfd8138773998305093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da3e4224710c44e3894306b363ea8a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91d45bfd41b1464db817d511512fef68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac1a5e4f34d24bedaa77a8ed3ab60203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68227bd9ce3545ba8a9fd6322f807a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "#Load pre-trained BERT model and tokenizer\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name)\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c8ee261-a0c3-45a7-b697-d4a266471219",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data\"\n",
    "path_to_real_file = os.path.join(data_path, \"llm-detect-ai-generated-text.zip\")\n",
    "path_to_extra_file1 = os.path.join(data_path, \"llm_data_one.zip\")\n",
    "\n",
    "with zipfile.ZipFile(path_to_real_file, 'r') as zip_ref1:\n",
    "    zip_ref1.extractall(data_path)\n",
    "with zipfile.ZipFile(path_to_extra_file1, 'r') as zip_ref2:\n",
    "    zip_ref2.extractall(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07073e5b-ad84-45e0-bdbf-f25cb8e3d453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset Class for Pre-Encoded Data\n",
    "class EncodedTextDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_masks, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_masks[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abd2994e-d211-4ace-aac5-f7224e1d1687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Encode the Texts\n",
    "def encode_texts(texts, labels, tokenizer, max_length=512):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for text in texts:\n",
    "        encoded_text = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids.append(encoded_text['input_ids'].flatten())\n",
    "        attention_masks.append(encoded_text['attention_mask'].flatten())\n",
    "\n",
    "    return torch.stack(input_ids), torch.stack(attention_masks), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e531a94-45e0-4aec-aff7-900edaa63192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the classification model\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        return self.classifier(pooled_output)\n",
    "\n",
    "model = TextClassifier(bert_model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "875e554b-8827-4235-9845-91fe48388ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "df = pd.read_csv(f'{data_path}/train_essays.csv')  # Read CSV file into a DataFrame\n",
    "df['label'] = df.generated.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d01c5a0-8252-49f2-a312-f9a8f986a397",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_files = [\"train_drcat_01.csv\", \"train_drcat_02.csv\", \"train_drcat_03.csv\", \"train_drcat_04.csv\"]\n",
    "df_extra = pd.concat(pd.read_csv(os.path.join(data_path, f)) for f in ex_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36ee6248-f591-4a4f-953e-851c24831eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.concat([df_extra[[\"text\", \"label\"]], df[[\"text\", \"label\"]]])\n",
    "# Encode texts and labels\n",
    "texts = df_data[\"text\"].values.tolist()\n",
    "labels = df_data[\"label\"].values.tolist()\n",
    "input_ids, attention_masks, labels = encode_texts(texts, labels, tokenizer)\n",
    "torch.save({\n",
    "    'input_ids': input_ids,\n",
    "    'attention_masks': attention_masks,\n",
    "    'labels': labels\n",
    "}, os.path.join(data_path, 'tensor_data_llm_fake.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "543abd0b-e50d-44c6-9bdd-317188b2bc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tensors from the file\n",
    "saved_tensors = torch.load(os.path.join(data_path, 'tensor_data_llm_fake.pth'))\n",
    "\n",
    "input_ids = saved_tensors['input_ids']\n",
    "attention_masks = saved_tensors['attention_masks']\n",
    "labels = saved_tensors['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11cc2c5b-5632-474a-9cff-24b986b4d860",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into training and testing sets\n",
    "train_ids, test_ids, train_masks, test_masks, train_labels, test_labels = train_test_split(\n",
    "    input_ids, attention_masks, labels, test_size=0.2\n",
    ")\n",
    "\n",
    "# Create Dataset and DataLoader for training and testing\n",
    "train_dataset = EncodedTextDataset(train_ids, train_masks, train_labels)\n",
    "test_dataset = EncodedTextDataset(test_ids, test_masks, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e245961c-5966-42c6-b9fc-05f2546e0e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Evaluation\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2306f36a-eea1-4433-b0a5-62f202d45f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            predictions = torch.round(torch.sigmoid(outputs.squeeze()))\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb549b7a-f53a-49e9-b747-b180a42ef113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction Function\n",
    "def predict(texts, model, tokenizer, max_length=512):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for text in texts:\n",
    "            encoded_text = tokenizer.encode_plus(\n",
    "                text,\n",
    "                add_special_tokens=True,\n",
    "                max_length=max_length,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt'\n",
    "            ).to(device)\n",
    "\n",
    "            input_ids = encoded_text['input_ids']\n",
    "            attention_mask = encoded_text['attention_mask']\n",
    "            output = model(input_ids, attention_mask)\n",
    "            prediction = torch.sigmoid(output).item()\n",
    "            predictions.append(prediction)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd515264-7568-4cc1-a9ab-bcc4228d30fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoches =6\n",
    "for epoch in range(epoches):\n",
    "    model.train()\n",
    "    n = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (n+1)%100 ==0:\n",
    "            print(f'step {n + 1}, train_loss: {loss.item():.2f}')\n",
    "        n +=1\n",
    "    # Evaluate on the testing set\n",
    "    test_accuracy = evaluate(model, test_loader)\n",
    "    print(f'Epoch {epoch + 1}, Test Accuracy: {test_accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3badb090-720b-4a76-9871-b9093d5fb562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Model\n",
    "model_dir = \"./model_dir\"\n",
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "# Load the Model\n",
    "def load_model(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "model_path = os.path.join(model_dir, \"bert_text_classifier.pth\")\n",
    "save_model(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a01788-b9ba-4df9-9be6-063c0adc2e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#submission \n",
    "# Read the test dataset from a CSV file\n",
    "test = pd.read_csv(os.path.join(data_dir, \"test_essays.csv\"))\n",
    "texts_sub = test[\"text\"].values.tolist()\n",
    "\n",
    "# Generate predictions for the test dataset using the trained model\n",
    "                   \n",
    "test[\"generated\"] = predict(texts_sub, model, tokenizer, max_length=512)\n",
    "# Create a submission dataframe with the required columns\n",
    "submission = test[[\"id\", \"generated\"]]\n",
    "# Save the submission dataframe to a CSV file\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "# Display the first few rows of the submission dataframe\n",
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
